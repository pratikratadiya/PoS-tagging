{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download() # Go to corpora tab and download brown and conll2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.']\n"
     ]
    }
   ],
   "source": [
    "sentences = brown.sents() # Going through the data of brown corpus and seeing a sample sentence\n",
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentences -> sentences upon which we will train in our case brown sentences\n",
    "# size -> embedding dimensions. Can take any value, usually around 300/500 preferred\n",
    "# window -> these many words on the left and these many words on the right will be considered as context\n",
    "# min_count -> Word should be present atleast these many times in corpus to be considered in our vocabulary\n",
    "# negative -> Number of samples to be considered for negative sampling\n",
    "# iter -> Number of iterations to train upon\n",
    "# workers -> Number of threads to be put to work\n",
    "emb_dim = 300\n",
    "w2v = Word2Vec(sentences,size=emb_dim,window=5,min_count=5,negative=15,iter=10,workers=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vectors = w2v.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15173, 300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar words are:\n",
      "  [('girl', 0.9193911552429199), ('woman', 0.8401344418525696), ('man', 0.7483998537063599)]\n"
     ]
    }
   ],
   "source": [
    "result = word_vectors.similar_by_word(\"boy\")\n",
    "print(\"Most similar words are:\\n \",result[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our own word embeddings ready which can be seen have produced quite good vectors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using these word embeddings for some task! Say Part of Speech tagging where in every word of the sentence is tagged with what Part of Speech it is. We will use the conll2000 dataset for this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import conll2000\n",
    "from keras.layers import Dense, Embedding, Activation, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Confidence', 'NN'), ('in', 'IN'), ('the', 'DT'), ('pound', 'NN'), ('is', 'VBZ')]\n"
     ]
    }
   ],
   "source": [
    "# Retrieving the train and test data and seeing a sample\n",
    "train_words = conll2000.tagged_words(\"train.txt\")\n",
    "test_words = conll2000.tagged_words(\"test.txt\")\n",
    "print(train_words[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will convert our data into a more model-friendly format. For doing the same, lets split the words from their tags. Then map each word to the index at which it is present in our word2vec. But first let's comvert all POS to unique integer ID's as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tag_vocabulary(tagged_words):\n",
    "    \"\"\"\n",
    "    Accepts text in the form of (word,pos) and returns a dictionary mapping all POS to unique id's\n",
    "    \"\"\"\n",
    "    tag2id = {}\n",
    "    for item in tagged_words:\n",
    "        tag = item[1]\n",
    "        tag2id.setdefault(tag, len(tag2id))\n",
    "    return tag2id\n",
    "\n",
    "# Right now our word2vec maps words to gensim objects but we would like a word to integer kind of mapping\n",
    "# so we map words to the object indexes by the below line\n",
    "word2id = {k:v.index for k,v in word_vectors.vocab.items()}\n",
    "tag2id = get_tag_vocabulary(train_words)  # Returning our POS to unique id dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all POS tagged to unique IDs, also we have a word2vec dictionary in the form of a word->int mapping. We can now map all our words in training dataset to these word2id. However there is one issue i.e. there can be certain words in the training dataset which aren't present in our dictionary. To all such words we can assign the unknown tag(UNK). To do so, we must first add a UNK entry in our word2vec dictionary. Let's do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_new_word(new_word,new_vector,new_index,embedding_matrix,word2id):\n",
    "    \"\"\"\n",
    "    Adds a new word to existing list of word embeddings\n",
    "    \"\"\"\n",
    "    # inserting vector before given index, along axis 0\n",
    "    embedding_matrix = np.insert(embedding_matrix, [new_index], [new_vector], axis=0)\n",
    "    # Appending indexes of all words after new word by 1 so as to accomodate new word\n",
    "    word2id = {word: (index+1) if index >= new_index else index for word, index in word2id.items()}\n",
    "    word2id[new_word] = new_index\n",
    "    return embedding_matrix, word2id\n",
    "\n",
    "UNK_index = 0 # we will keep the unknown word at index 0\n",
    "UNK_token = \"UNK\"\n",
    "\n",
    "embedding_matrix = word_vectors.vectors\n",
    "unk_vector = embedding_matrix.mean(0) # The value of UNK vector is average of all vectors\n",
    "embedding_matrix, word2id = add_new_word(UNK_token,unk_vector, UNK_index, embedding_matrix,word2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets associate all our training words with our word2id dictionary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data created. Unknown data percentage: 0.143\n",
      "Data created. Unknown data percentage: 0.149\n"
     ]
    }
   ],
   "source": [
    "def get_int_data(tagged_words, word2id, tag2id):\n",
    "    \"\"\"\n",
    "    Replace all words with their corresponding ids from our dictionary\n",
    "    \"\"\"\n",
    "    x, y = [], []\n",
    "    unk_count = 0\n",
    "    \n",
    "    for word, tag in tagged_words:\n",
    "        y.append(tag2id.get(tag))\n",
    "        if word in word2id:\n",
    "            x.append(word2id.get(word))\n",
    "        else:\n",
    "            x.append(UNK_index)\n",
    "            unk_count += 1\n",
    "    print(\"Data created. Unknown data percentage: %.3f\" % (unk_count/len(tagged_words)))\n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "x_train, y_train = get_int_data(train_words, word2id, tag2id)\n",
    "x_test, y_test = get_int_data(test_words, word2id, tag2id)\n",
    "y_train, y_test = to_categorical(y_train), to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have model friendly data. Let's define our model to classify words now. Our model will take as input an index into the word embedding matrix, which will be used to look up the appropriate embedding. It will have one hidden layer with the tanh activation function and at the final layer will use the softmax activation — outputting a probability distribution over all possible tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pratik/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1, 300)            4552200   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                15050     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 44)                2244      \n",
      "=================================================================\n",
      "Total params: 4,569,494\n",
      "Trainable params: 4,569,494\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 50\n",
    "batch_size = 128\n",
    "\n",
    "def define_model(embedding_matrix, class_count):\n",
    "    \"\"\"\n",
    "    Takes one word as input and returns its part of speech\n",
    "    \"\"\"\n",
    "    vocab_length = len(embedding_matrix)\n",
    "    model = Sequential()\n",
    "    # Input dimension would be length of our vocabulary, output would be 300 dimensional embedding\n",
    "    # We load our pretrainned word2vec weights and set the input size to be 1\n",
    "    model.add(Embedding(input_dim=vocab_length,output_dim=300,weights=[embedding_matrix],input_length=1))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(hidden_size,activation=\"tanh\"))\n",
    "    model.add(Dense(class_count,activation=\"softmax\"))\n",
    "    model.compile(optimizer=\"Adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "pos_model = define_model(embedding_matrix, len(tag2id))\n",
    "pos_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/pratik/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n",
      "211727/211727 [==============================] - 156s 739us/step - loss: 0.6612 - acc: 0.8190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f25443e7f98>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_model.fit(x_train,y_train,epochs=1,verbose=1,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47377/47377 [==============================] - 1s 26us/step\n",
      "Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "_, acc = pos_model.evaluate(x_test, y_test)\n",
    "print(\"Accuracy: %.2f\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila! 85% accuracy on the testing dataset with our model which passed one word at a time. What if we passed multiple words at a time to get some context?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Something like {word1,word2,word3,word4,word5} and for this corresponding index passed will be of word3. So we could say in a way that we are passing word3 but along with it 2 neighboring words on either sides to our model. We will need to make certain changes in our model for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Also, we will need a token to be used for padding in case there are no 2 neighboring words present. We call this the EOS(end of sentence) word. We will add it to our dictionary similar to the UNK that we added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eos_index = 1\n",
    "eos_tag = \"EOS\"\n",
    "# Vector value will be kept random\n",
    "eos_vector = np.random.standard_normal(300) # of embedding dimension size 300\n",
    "embedding_matrix, word2id = add_new_word(eos_tag,eos_vector,eos_index,embedding_matrix,word2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare our context dependent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context_size = 2\n",
    "\n",
    "def get_window_int_data(tagged_words, word2id, tag2id):\n",
    "    x,y = [], []\n",
    "    unk_count = 0\n",
    "    \n",
    "    span = 2*context_size+1 # total 5 words are being considered\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    padding = [(eos_tag, None)] * context_size\n",
    "    buffer += padding + tagged_words[:context_size]\n",
    "    \n",
    "    for item in (tagged_words[context_size:] + padding):\n",
    "        buffer.append(item)\n",
    "        window_ids = np.array([word2id.get(word) if (word in word2id) else UNK_index for (word,_) in buffer])\n",
    "        x.append(window_ids)\n",
    "        \n",
    "        middle_word, middle_tag = buffer[context_size]\n",
    "        y.append(tag2id.get(middle_tag))\n",
    "        \n",
    "        if middle_word not in word2id:\n",
    "            unk_count += 1\n",
    "            \n",
    "    print(\"Data created. Percentage of unknown words: %.3f\" % (unk_count/len(tagged_words)))\n",
    "    return np.array(x),np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our model now which does the training part. Only thing that changes is embedding layer now takes input size as 5 and not 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_context_model(embedding_matrix, class_count):\n",
    "    \"\"\"\n",
    "    Takes word and its context as input and returns its part of speech\n",
    "    \"\"\"\n",
    "    inp = 2*context_size + 1\n",
    "    vocab_length = len(embedding_matrix)\n",
    "    model = Sequential()\n",
    "    # Input dimension would be length of our vocabulary, output would be 300 dimensional embedding\n",
    "    # We load our pretrainned word2vec weights and set the input size to be 5\n",
    "    model.add(Embedding(input_dim=vocab_length,output_dim=300,weights=[embedding_matrix],input_length=inp))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(hidden_size,activation=\"tanh\"))\n",
    "    model.add(Dense(class_count,activation=\"softmax\"))\n",
    "    model.compile(optimizer=\"Adam\",loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data created. Percentage of unknown words: 0.143\n",
      "Data created. Percentage of unknown words: 0.149\n"
     ]
    }
   ],
   "source": [
    "x_train2, y_train2 = get_window_int_data(train_words, word2id, tag2id)\n",
    "x_test2, y_test2 = get_window_int_data(test_words, word2id, tag2id)\n",
    "y_train2, y_test2 = to_categorical(y_train2), to_categorical(y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "211727/211727 [==============================] - 156s 736us/step - loss: 0.4878 - acc: 0.8658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2551303cc0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs_pos_model = define_context_model(embedding_matrix,len(tag2id))\n",
    "cs_pos_model.fit(x_train2,y_train2,batch_size=batch_size,epochs=1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47377/47377 [==============================] - 3s 73us/step\n",
      "Accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "_, acc2 = cs_pos_model.evaluate(x_test2,y_test2)\n",
    "print(\"Accuracy: %.2f\" % acc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "91% accuracy holy molly hell!! Just by training on 1 epoch and with 2 context words on each side. Not to forget that we used our very own derived embeddings for this divine cause. Fcking party time! But before that, saving the model with their weights to avoid retraining in the future :1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_model.save('single_word_85test.h5')\n",
    "cs_pos_model.save('context_word_91test.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Milte hai agle episode mai, asta la vista!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
